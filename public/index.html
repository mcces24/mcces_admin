<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Detection and Recognition</title>
  <style>
    video {
      border: 1px solid black;
      width: 50%;
      height: auto;
    }
    canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
  </style>
</head>
<body>
  <img id="inputImage" src="other.jpg" alt="Face for recognition" width="400"/>
  <video id="video" width="640" height="480" autoplay></video>
  <canvas id="canvas" width="640" height="480"></canvas>
  <button id="startDetection">Start Face Detection</button>

  <!-- Load face-api.js from CDN -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const startDetectionButton = document.getElementById('startDetection');

    // Store known face embeddings
    let knownFaceEmbeddings = [];

    // Load face-api.js models
    async function loadModels() {
      try {
        console.log("Loading models...");
        await faceapi.nets.ssdMobilenetv1.loadFromUri('/models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
        await faceapi.nets.faceRecognitionNet.loadFromUri('/models');
        console.log("Models loaded successfully");
      } catch (error) {
        console.error("Error loading models:", error);
      }
    }

    // Access webcam feed
    navigator.mediaDevices.getUserMedia({
      video: true
    }).then(stream => {
      video.srcObject = stream;
    }).catch(err => {
      console.log("Error accessing webcam:", err);
    });

    // Start face detection when button is clicked
    startDetectionButton.addEventListener('click', async () => {
      await loadModels();  // Ensure models are loaded before starting detection
      detectFace();        // Call detectFace after models are loaded
    });

    // Load the image and extract embeddings for known faces

  async function loadFaceApi() {
      await faceapi.nets.ssdMobilenetv1.loadFromUri('/models');
      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
      await faceapi.nets.faceRecognitionNet.loadFromUri('/models');

      // Step 1: Store known face embeddings
      // Example: Adding a known face (name and image)
      const image = document.getElementById('inputImage');
      const detections = await faceapi.detectAllFaces(image)
        .withFaceLandmarks()
        .withFaceDescriptors();
      
      // Get the first face descriptor (embedding)
      const embedding = detections[0]?.descriptor;

      // Store the embedding with the name
      if (embedding) {
        const name = 'John Doe'; // Replace with actual name
        knownFaceEmbeddings.push({ name, embedding });
      }

      console.log('Known face embeddings:', knownFaceEmbeddings);
      // Step 2: Perform face recognition (for demo purposes)

    }

    // Compare embeddings to recognize faces
    function compareEmbeddings(embedding1, embedding2) {
      const distance = faceapi.euclideanDistance(embedding1, embedding2);
      return distance < 0.6; // Adjust threshold based on model's accuracy
    }

    // Recognize face based on embeddings
    async function recognizeFace(detection) {
      const detectedEmbedding = detection.descriptor;
      
      for (let knownFace of knownFaceEmbeddings) {
        if (compareEmbeddings(detectedEmbedding, knownFace.embedding)) {
          console.log("Face recognized as", knownFace.name);
          return knownFace.name;
        }
      }
      console.log("Face not recognized");
      return "Unknown";
    }

    // Detect faces in webcam and try to recognize them
    async function detectFace() {
      const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors();

      ctx.drawImage(video, 0, 0, canvas.width, canvas.height); // Draw video frame on canvas

      detections.forEach(async detection => {
        const start = detection.detection.box.topLeft;
        const end = detection.detection.box.bottomRight;
        const width = end.x - start.x;
        const height = end.y - start.y;

        // Draw bounding box around face
        ctx.beginPath();
        ctx.rect(start.x, start.y, width, height);
        ctx.lineWidth = 4;
        ctx.strokeStyle = 'red';
        ctx.fillStyle = 'red';
        ctx.stroke();
        ctx.fillText("Face", start.x, start.y - 10);

        // Recognize face
        const recognizedName = await recognizeFace(detection);
        ctx.fillText(recognizedName, start.x, start.y - 25);
      });

      requestAnimationFrame(detectFace); // Continuously detect faces
    }

    // Load known faces (replace with actual image paths)
    loadFaceApi();


  </script>
</body>
</html>
